{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-25 라이브러리 호출\n",
    "from __future__ import unicode_literals, print_function, division  # ①\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re  # ②\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-26 데이터 준비\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "class Lang:  # 딕셔너리를 만들기 위한 클래스\n",
    "    def __init__(self):  # 단어의 인덱스를 저장하기 위한 컨테이너를 초기화\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            0: \"SOS\",\n",
    "            1: \"EOS\",\n",
    "        }  # SOS(Start Of Sequence): 문장의 시작, EOS(End Of Sequence): 문장의 끝\n",
    "        self.n_words = 2  # SOS와 EOS에 대한 카운트\n",
    "\n",
    "    def addSentence(\n",
    "        self, sentence\n",
    "    ):  # 문장을 단어 단위로 분리한 후 컨테이너(word)에 추가\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(\n",
    "        self, word\n",
    "    ):  # 컨테이너에 단어가 없다면 추가되고, 있다면 카운트를 업데이트\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-27 데이터 정규화\n",
    "def normalizeString(df, lang):\n",
    "    sentence = df[lang].str.lower()  # 소문자로 전환\n",
    "    sentence = sentence.str.replace(\n",
    "        \"[^A-Za-z\\s]+\", \" \"\n",
    "    )  # a-z, A-Z, …, ?, ! 등을 제외하고 모두 공백으로 바꿈\n",
    "    sentence = sentence.str.normalize(\"NFD\")  # 유니코드 정규화 방식\n",
    "    sentence = sentence.str.encode(\"ascii\", errors=\"ignore\").str.decode(\n",
    "        \"utf-8\"\n",
    "    )  # Unicode를 ASCII로 전환\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    sentence1 = normalizeString(df, lang1)  # 데이터셋의 첫 번째 열(영어)\n",
    "    sentence2 = normalizeString(df, lang2)  # 데이터셋의 두 번째 열(프랑스어)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter=\"\\t\", header=None, names=[lang1, lang2])  # ①\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(lang1, lang2):  # 데이터셋 불러오기\n",
    "    df = read_file(\"../chap10/data/%s-%s.txt\" % (lang1, lang2), lang1, lang2)\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if (\n",
    "            len(sentence1[i].split(\" \")) < MAX_LENGTH\n",
    "            and len(sentence2[i].split(\" \")) < MAX_LENGTH\n",
    "        ):\n",
    "            full = [sentence1[i], sentence2[i]]  # 첫 번째와 두 번째 열을 합쳐서 저장\n",
    "            input_lang.addSentence(sentence1[i])  # 입력(input)으로 영어를 사용\n",
    "            output_lang.addSentence(sentence2[i])  # 출력(output)으로 프랑스어를 사용\n",
    "            pairs.append(full)  # pairs에는 입력과 출력이 합쳐진 것을 사용\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-28 텐서로 변환\n",
    "def indexesFromSentence(lang, sentence):  # 문장을 단어로 분리하고 인덱스를 반환\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensorFromSentence(\n",
    "    lang, sentence\n",
    "):  # 딕셔너리에서 단어에 대한 인덱스를 가져오고 문장 끝에 토큰을 추가\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(\n",
    "    input_lang, output_lang, pair\n",
    "):  # 입력과 출력 문장을 텐서로 변환하여 반환\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-29 인코더 네트워크\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim  # 인코더에서 사용할 입력층\n",
    "        self.embbed_dim = embbed_dim  # 인코더에서 사용할 임베딩 계층\n",
    "        self.hidden_dim = hidden_dim  # 인코더에서 사용할 은닉층(이전 은닉층)\n",
    "        self.num_layers = num_layers  # 인코더에서 사용할 GRU의 계층 개수\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)  # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(\n",
    "            self.embbed_dim, self.hidden_dim, num_layers=self.num_layers\n",
    "        )  # 임베딩 차원, 은닉층 차원, GRU의 계층 개수를 이용하여 GRU 계층을 초기화\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(1, 1, -1)  # 임베딩 처리\n",
    "        outputs, hidden = self.gru(embedded)  # 임베딩 결과를 GRU 모델에 적용\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-30 디코더 네트워크\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)  # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(\n",
    "            self.embbed_dim, self.hidden_dim, num_layers=self.num_layers\n",
    "        )  # GRU 계층 초기화\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)  # 선형 계층 초기화\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  # ①\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1)  # 입력을 (1, 배치 크기)로 변경\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-31 seq2seq 네트워크\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder  # 인코더 초기화\n",
    "        self.decoder = decoder  # 디코더 초기화\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
    "        input_length = input_lang.size(0)  # 입력 문자 길이(문장의 단어 수)\n",
    "        batch_size = output_lang.shape[1]\n",
    "        target_length = output_lang.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(\n",
    "            self.device\n",
    "        )  # 예측된 출력을 저장하기 위한 변수 초기화\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                input_lang[i]\n",
    "            )  # 문장의 모든 단어를 인코딩\n",
    "        decoder_hidden = encoder_hidden.to(\n",
    "            device\n",
    "        )  # 인코더의 은닉층을 디코더의 은닉층으로 사용\n",
    "        decoder_input = torch.tensor(\n",
    "            [SOS_token], device=device\n",
    "        )  # 첫 번째 예측 단어 앞에 토큰(SOS) 추가\n",
    "\n",
    "        for t in range(target_length):  # 현재 단어에서 출력 단어를 예측\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio  # ①\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input = (\n",
    "                output_lang[t] if teacher_force else topi\n",
    "            )  # teacher_force를 활성화하면 목표 단어를 다음 입력으로 사용\n",
    "            if (\n",
    "                teacher_force == False and input.item() == EOS_token\n",
    "            ):  # teacher_force를 활성화하지 않으면 자체 예측 값을 다음 입력으로 사용\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-32 모델의 오차 계산 함수 정의\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    output = model(input_tensor, target_tensor)\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(\n",
    "            output[ot], target_tensor[ot]\n",
    "        )  # 모델의 예측 결과와 정답(예상 결과)을 이용하여 오차를 계산\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-33 모델 훈련 함수 정의\n",
    "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)  # 옵티마이저로 SGD를 사용\n",
    "    criterion = nn.NLLLoss()  # ①\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [\n",
    "        tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
    "        for i in range(num_iteration)\n",
    "    ]\n",
    "\n",
    "    for iter in range(1, num_iteration + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(\n",
    "            model, input_tensor, target_tensor, optimizer, criterion\n",
    "        )  # Model 객체를 이용하여 오차 계산\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if iter % 5000 == 0:  # 5000번째마다 오차 값에 대해 출력\n",
    "            average_loss = total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            print(\"%d %.4f\" % (iter, average_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), \"../chap10/data/mytraining.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-34 모델 평가\n",
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(\n",
    "            input_lang, sentences[0]\n",
    "        )  # 입력 문자열을 텐서로 변환\n",
    "        output_tensor = tensorFromSentence(\n",
    "            output_lang, sentences[1]\n",
    "        )  # 출력 문자열을 텐서로 변환\n",
    "        decoded_words = []\n",
    "        output = model(input_tensor, output_tensor)\n",
    "\n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(\n",
    "                1\n",
    "            )  # 각 출력에서 가장 높은 값을 찾아 인덱스를 반환\n",
    "\n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")  # EOS 토큰을 만나면 평가를 멈춥니다.\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(\n",
    "                    output_lang.index2word[topi[0].item()]\n",
    "                )  # 예측 결과를 출력 문자열에 추가\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateRandomly(\n",
    "    model, input_lang, output_lang, pairs, n=10\n",
    "):  # 훈련 데이터셋으로부터 임의의 문장을 가져와서 모델 평가\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)  # 임의로 문장을 가져옵니다.\n",
    "        print(\"input {}\".format(pair[0]))\n",
    "        print(\"output {}\".format(pair[1]))\n",
    "        output_words = evaluate(\n",
    "            model, input_lang, output_lang, pair\n",
    "        )  # 모델 평가 결과는 output_words에 저장\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"predicted {}\".format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['let me tell you everything i know.', 'laissez-moi vous conter tout ce que je sais.']\n",
      "Input : 23191 Output : 39387\n",
      "Encoder(\n",
      "  (embedding): Embedding(23191, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(39387, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=39387, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "5000 5.0150\n",
      "10000 4.8061\n",
      "15000 4.7233\n",
      "20000 4.6867\n",
      "25000 4.6432\n",
      "30000 4.6151\n",
      "35000 4.6661\n",
      "40000 4.6916\n",
      "45000 4.6349\n",
      "50000 4.6210\n",
      "55000 4.5618\n",
      "60000 4.5624\n",
      "65000 4.5336\n",
      "70000 4.5871\n",
      "75000 4.6190\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-35 모델 훈련\n",
    "lang1 = \"eng\"  # 입력으로 사용할 영어\n",
    "lang2 = \"fra\"  # 출력으로 사용할 프랑스어\n",
    "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print(\"random sentence {}\".format(randomize))\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print(\n",
    "    \"Input : {} Output : {}\".format(input_size, output_size)\n",
    ")  # 입력과 출력에 대한 단어 수 출력\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 75000  # 7만 5000번 반복하여 모델 훈련\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size, hidden_size, embed_size, num_layers\n",
    ")  # 인코더에 훈련 데이터셋을 입력하고 모든 출력과 은닉 상태를 저장\n",
    "decoder = Decoder(\n",
    "    output_size, hidden_size, embed_size, num_layers\n",
    ")  # 디코더의 첫 번째 입력으로 <SOS> 토큰이 제공되고, 인코더의 마지막 은닉 상태가 디코더의 첫 번째 은닉 상태로 제공됩니다.\n",
    "model = Seq2Seq(encoder, decoder, device).to(\n",
    "    device\n",
    ")  # 인코더-디코더 모델(seq2seq)의 객체 생성\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, input_lang, output_lang, pairs, num_iteration)  # 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input don't let your mother know that.\n",
      "output que ta mere n'en sache rien !\n",
      "predicted je ne pas pas <EOS>\n",
      "input why are you picking a fight with me?\n",
      "output pourquoi me cherchez-vous querelle ?\n",
      "predicted je ne pas pas <EOS>\n",
      "input he was unconscious of his guilt.\n",
      "output il etait inconscient de sa culpabilite.\n",
      "predicted je ne pas pas <EOS>\n",
      "input do you still want me to give tom your old computer?\n",
      "output veux-tu toujours que je donne ton ancien ordinateur a tom ?\n",
      "predicted je ne pas pas <EOS>\n",
      "input i wish i had a friend like you.\n",
      "output j'aimerais avoir une amie telle que vous.\n",
      "predicted je ne pas pas <EOS>\n",
      "input i love to party.\n",
      "output j'adore faire la java.\n",
      "predicted je ne pas pas <EOS>\n",
      "input could you take our picture?\n",
      "output pourrais-tu nous prendre en photo ?\n",
      "predicted je ne pas pas <EOS>\n",
      "input the telephone is ringing, but nobody is answering.\n",
      "output le telephone sonne mais personne ne repond.\n",
      "predicted je ne pas pas <EOS>\n",
      "input are you sure that you haven't forgotten anything?\n",
      "output etes-vous sur de n'avoir rien oublie ?\n",
      "predicted je ne pas pas <EOS>\n",
      "input i agree with you absolutely.\n",
      "output je suis tout a fait d'accord avec toi.\n",
      "predicted je ne pas pas <EOS>\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-36 임의의 문장에 대한 평가 결과\n",
    "evaluateRandomly(model, input_lang, output_lang, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-37 어텐션이 적용된 디코더\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.output_size, self.hidden_size\n",
    "        )  # 임베딩 계층 초기화\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)  # ①\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)\n",
    "        )  # ②\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-38 어텐션 디코더 모델 학습을 위한 함수\n",
    "def trainIters(\n",
    "    encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(\n",
    "        encoder.parameters(), lr=learning_rate\n",
    "    )  # 인코더와 디코더에 SGD 옵티마이저 적용\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [\n",
    "        tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
    "        for i in range(n_iters)\n",
    "    ]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]  # 입력+출력 쌍에서 입력을 input_tensor로 사용\n",
    "        target_tensor = training_pair[1]  # 입력+출력 쌍에서 출력을 target_tensor로 사용\n",
    "        loss = Model(model, input_tensor, target_tensor, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if (\n",
    "            iter % 5000 == 0\n",
    "        ):  # 모델을 7만 5000번 훈련을 진행하며 5000번째마다 오차를 출력\n",
    "            print_loss_avg = print_loss_total / 5000\n",
    "            print_loss_total = 0\n",
    "            print(\"%d, %.4f\" % (iter, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(23191, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(39387, 512)\n",
      "  (attn): Linear(in_features=1024, out_features=20, bias=True)\n",
      "  (attn_combine): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gru): GRU(512, 512)\n",
      "  (out): Linear(in_features=512, out_features=39387, bias=True)\n",
      ")\n",
      "5000, 4.9127\n",
      "10000, 4.9277\n",
      "15000, 4.9455\n",
      "20000, 4.9397\n",
      "25000, 4.9240\n",
      "30000, 4.9247\n",
      "35000, 4.9373\n",
      "40000, 4.9325\n",
      "45000, 4.9231\n",
      "50000, 4.9487\n",
      "55000, 4.9132\n",
      "60000, 4.9453\n",
      "65000, 4.8964\n",
      "70000, 4.9250\n",
      "75000, 4.9165\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-39 어텐션 디코더 모델 훈련\n",
    "import time\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "\n",
    "encoder1 = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
    "print(encoder1)\n",
    "print(attn_decoder1)\n",
    "\n",
    "attn_model = trainIters(\n",
    "    encoder1, attn_decoder1, 75000, print_every=5000, plot_every=100, learning_rate=0.01\n",
    ")  # 인코더와 어텐션 디코더를 이용한 모델 생성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
