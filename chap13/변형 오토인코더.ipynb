{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /home/halozhan/딥러닝 파이토치 교과서/.venv/lib/python3.8/site-packages (from tensorboardX) (4.25.5)\n",
      "Requirement already satisfied: packaging in /home/halozhan/딥러닝 파이토치 교과서/.venv/lib/python3.8/site-packages (from tensorboardX) (24.2)\n",
      "Requirement already satisfied: numpy in /home/halozhan/딥러닝 파이토치 교과서/.venv/lib/python3.8/site-packages (from tensorboardX) (1.24.3)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.6.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-11 필요한 라이브러리 호출\n",
    "import datetime\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-12 데이터셋을 내려받은 후 텐서 변환\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"../chap13/data\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"../chap13/data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=100, shuffle=True, num_workers=4, pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-13 인코더 네트워크 생성\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.input1(x))\n",
    "        h_ = self.LeakyReLU(self.input2(h_))\n",
    "        mean = self.mean(h_)\n",
    "        log_var = self.var(h_)\n",
    "        return mean, log_var  # 인코더 네트워크에서 평균과 분산을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-14 디코더 네트워크\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.hidden1(x))\n",
    "        h = self.LeakyReLU(self.hidden2(h))\n",
    "        x_hat = torch.sigmoid(self.output(h))\n",
    "        return x_hat  # 디코더 결과는 시그모이드를 통과했으므로 0~1 값을 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-15 변형 오토인코더 네트워크\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "\n",
    "    def reparameterization(self, mean, var):  # ①\n",
    "        epsilon = torch.randn_like(var).to(device)\n",
    "        z = mean + var * epsilon  # z 값 구하기\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)  # ②\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.Decoder(z)\n",
    "        return (\n",
    "            x_hat,\n",
    "            mean,\n",
    "            log_var,\n",
    "        )  # 디코더 결과와 평균, 표준편차(log를 취한 표준편차)를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-16 인코더와 디코더 객체 초기화\n",
    "x_dim = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-17 손실 함수 정의\n",
    "def loss_function(x, x_hat, mean, log_var):  # ①\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss, KLD\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-18 모델 학습 함수 정의\n",
    "saved_loc = \"scalar/\"  # 텐서보드에서 사용할 경로\n",
    "writer = SummaryWriter(saved_loc)  # ①\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "def train(epoch, model, train_loader, optimizer):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "        loss = BCE + KLD\n",
    "        writer.add_scalar(\n",
    "            \"Train/Reconstruction Error\",\n",
    "            BCE.item(),\n",
    "            batch_idx + epoch * (len(train_loader.dataset) / batch_size),\n",
    "        )  # ②\n",
    "        writer.add_scalar(\n",
    "            \"Train/KL-Divergence\",\n",
    "            KLD.item(),\n",
    "            batch_idx + epoch * (len(train_loader.dataset) / batch_size),\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            \"Train/Total Loss\",\n",
    "            loss.item(),\n",
    "            batch_idx + epoch * (len(train_loader.dataset) / batch_size),\n",
    "        )\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(x),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(x),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        \"======> Epoch: {} Average loss: {:.4f}\".format(\n",
    "            epoch, train_loss / len(train_loader.dataset)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 13-19 모델 평가 함수 정의\n",
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, _) in enumerate(test_loader):\n",
    "            x = x.view(batch_size, x_dim)\n",
    "            x = x.to(device)\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "            loss = BCE + KLD\n",
    "\n",
    "            writer.add_scalar(\n",
    "                \"Test/Reconstruction Error\",\n",
    "                BCE.item(),\n",
    "                batch_idx + epoch * (len(test_loader.dataset) / batch_size),\n",
    "            )  # 테스트 데이터셋에 대해서도 오차를 로그에 저장\n",
    "            writer.add_scalar(\n",
    "                \"Test/KL-Divergence\",\n",
    "                KLD.item(),\n",
    "                batch_idx + epoch * (len(test_loader.dataset) / batch_size),\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Test/Total Loss\",\n",
    "                loss.item(),\n",
    "                batch_idx + epoch * (len(test_loader.dataset) / batch_size),\n",
    "            )\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                n = min(x.size(0), 8)\n",
    "                comparison = torch.cat([x[:n], x_hat.view(batch_size, x_dim)[:n]])\n",
    "                grid = torchvision.utils.make_grid(comparison.cpu())\n",
    "                writer.add_image(\n",
    "                    \"Test image - Above: Real data, below: reconstruction data\",\n",
    "                    grid,\n",
    "                    epoch,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78eea068c38145df93d3ec00c01c17bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\t Loss: 545.295156\n",
      "Train Epoch: 0 [10000/60000 (17%)]\t Loss: 194.420605\n",
      "Train Epoch: 0 [20000/60000 (33%)]\t Loss: 187.118809\n",
      "Train Epoch: 0 [30000/60000 (50%)]\t Loss: 160.012686\n",
      "Train Epoch: 0 [40000/60000 (67%)]\t Loss: 161.440273\n",
      "Train Epoch: 0 [50000/60000 (83%)]\t Loss: 149.570811\n",
      "======> Epoch: 0 Average loss: 173.6872\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\t Loss: 139.700586\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t Loss: 139.730322\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t Loss: 124.656973\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t Loss: 123.389258\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t Loss: 122.993828\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t Loss: 124.900615\n",
      "======> Epoch: 1 Average loss: 128.0428\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\t Loss: 121.788740\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t Loss: 118.400615\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t Loss: 116.927939\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t Loss: 117.731973\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t Loss: 118.080342\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t Loss: 109.851328\n",
      "======> Epoch: 2 Average loss: 116.9011\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\t Loss: 114.031973\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t Loss: 116.131445\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t Loss: 117.169043\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t Loss: 111.393867\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t Loss: 112.365225\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t Loss: 111.295039\n",
      "======> Epoch: 3 Average loss: 112.8362\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\t Loss: 107.245459\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t Loss: 116.396777\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t Loss: 109.031680\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t Loss: 113.403926\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t Loss: 107.734902\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t Loss: 113.827285\n",
      "======> Epoch: 4 Average loss: 110.1195\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\t Loss: 104.638584\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t Loss: 105.921602\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t Loss: 111.418760\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t Loss: 106.011455\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t Loss: 106.415957\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t Loss: 110.823047\n",
      "======> Epoch: 5 Average loss: 108.2252\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\t Loss: 105.885547\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t Loss: 109.305352\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t Loss: 115.468789\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t Loss: 103.973271\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t Loss: 111.949062\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t Loss: 108.361484\n",
      "======> Epoch: 6 Average loss: 106.9864\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\t Loss: 109.325469\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t Loss: 106.889844\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t Loss: 102.626621\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t Loss: 107.513418\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t Loss: 106.239131\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t Loss: 100.073984\n",
      "======> Epoch: 7 Average loss: 106.0467\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\t Loss: 106.734922\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t Loss: 106.237637\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t Loss: 105.106348\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t Loss: 108.222041\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t Loss: 104.878379\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t Loss: 102.447021\n",
      "======> Epoch: 8 Average loss: 105.2434\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\t Loss: 109.772891\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t Loss: 108.834531\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t Loss: 101.475381\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t Loss: 106.007598\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t Loss: 101.139541\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t Loss: 110.902549\n",
      "======> Epoch: 9 Average loss: 104.6545\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\t Loss: 105.818916\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t Loss: 106.074346\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t Loss: 97.760820\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t Loss: 102.461699\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t Loss: 102.060918\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t Loss: 104.453789\n",
      "======> Epoch: 10 Average loss: 104.1132\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\t Loss: 102.211270\n",
      "Train Epoch: 11 [10000/60000 (17%)]\t Loss: 101.998242\n",
      "Train Epoch: 11 [20000/60000 (33%)]\t Loss: 104.515234\n",
      "Train Epoch: 11 [30000/60000 (50%)]\t Loss: 106.679512\n",
      "Train Epoch: 11 [40000/60000 (67%)]\t Loss: 103.205137\n",
      "Train Epoch: 11 [50000/60000 (83%)]\t Loss: 101.353799\n",
      "======> Epoch: 11 Average loss: 103.6685\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\t Loss: 104.121885\n",
      "Train Epoch: 12 [10000/60000 (17%)]\t Loss: 102.314043\n",
      "Train Epoch: 12 [20000/60000 (33%)]\t Loss: 103.930215\n",
      "Train Epoch: 12 [30000/60000 (50%)]\t Loss: 103.101768\n",
      "Train Epoch: 12 [40000/60000 (67%)]\t Loss: 107.191270\n",
      "Train Epoch: 12 [50000/60000 (83%)]\t Loss: 106.201729\n",
      "======> Epoch: 12 Average loss: 103.3326\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\t Loss: 100.572949\n",
      "Train Epoch: 13 [10000/60000 (17%)]\t Loss: 104.037305\n",
      "Train Epoch: 13 [20000/60000 (33%)]\t Loss: 103.162344\n",
      "Train Epoch: 13 [30000/60000 (50%)]\t Loss: 103.736191\n",
      "Train Epoch: 13 [40000/60000 (67%)]\t Loss: 105.252480\n",
      "Train Epoch: 13 [50000/60000 (83%)]\t Loss: 104.168369\n",
      "======> Epoch: 13 Average loss: 102.9523\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\t Loss: 97.348955\n",
      "Train Epoch: 14 [10000/60000 (17%)]\t Loss: 104.782188\n",
      "Train Epoch: 14 [20000/60000 (33%)]\t Loss: 102.613223\n",
      "Train Epoch: 14 [30000/60000 (50%)]\t Loss: 104.498164\n",
      "Train Epoch: 14 [40000/60000 (67%)]\t Loss: 103.535283\n",
      "Train Epoch: 14 [50000/60000 (83%)]\t Loss: 104.826807\n",
      "======> Epoch: 14 Average loss: 102.6387\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\t Loss: 101.338984\n",
      "Train Epoch: 15 [10000/60000 (17%)]\t Loss: 101.672754\n",
      "Train Epoch: 15 [20000/60000 (33%)]\t Loss: 106.498984\n",
      "Train Epoch: 15 [30000/60000 (50%)]\t Loss: 107.749053\n",
      "Train Epoch: 15 [40000/60000 (67%)]\t Loss: 101.198252\n",
      "Train Epoch: 15 [50000/60000 (83%)]\t Loss: 99.533369\n",
      "======> Epoch: 15 Average loss: 102.3522\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\t Loss: 104.185840\n",
      "Train Epoch: 16 [10000/60000 (17%)]\t Loss: 99.063027\n",
      "Train Epoch: 16 [20000/60000 (33%)]\t Loss: 96.379219\n",
      "Train Epoch: 16 [30000/60000 (50%)]\t Loss: 102.553770\n",
      "Train Epoch: 16 [40000/60000 (67%)]\t Loss: 102.717617\n",
      "Train Epoch: 16 [50000/60000 (83%)]\t Loss: 102.352187\n",
      "======> Epoch: 16 Average loss: 102.1319\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\t Loss: 104.028086\n",
      "Train Epoch: 17 [10000/60000 (17%)]\t Loss: 100.955898\n",
      "Train Epoch: 17 [20000/60000 (33%)]\t Loss: 97.669590\n",
      "Train Epoch: 17 [30000/60000 (50%)]\t Loss: 104.912783\n",
      "Train Epoch: 17 [40000/60000 (67%)]\t Loss: 100.152129\n",
      "Train Epoch: 17 [50000/60000 (83%)]\t Loss: 101.894277\n",
      "======> Epoch: 17 Average loss: 101.8837\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\t Loss: 103.112334\n",
      "Train Epoch: 18 [10000/60000 (17%)]\t Loss: 97.346387\n",
      "Train Epoch: 18 [20000/60000 (33%)]\t Loss: 101.456133\n",
      "Train Epoch: 18 [30000/60000 (50%)]\t Loss: 102.944102\n",
      "Train Epoch: 18 [40000/60000 (67%)]\t Loss: 99.716318\n",
      "Train Epoch: 18 [50000/60000 (83%)]\t Loss: 99.940781\n",
      "======> Epoch: 18 Average loss: 101.6438\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\t Loss: 101.398691\n",
      "Train Epoch: 19 [10000/60000 (17%)]\t Loss: 101.123867\n",
      "Train Epoch: 19 [20000/60000 (33%)]\t Loss: 104.572803\n",
      "Train Epoch: 19 [30000/60000 (50%)]\t Loss: 98.822002\n",
      "Train Epoch: 19 [40000/60000 (67%)]\t Loss: 101.946895\n",
      "Train Epoch: 19 [50000/60000 (83%)]\t Loss: 103.314551\n",
      "======> Epoch: 19 Average loss: 101.5050\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\t Loss: 105.335693\n",
      "Train Epoch: 20 [10000/60000 (17%)]\t Loss: 101.637324\n",
      "Train Epoch: 20 [20000/60000 (33%)]\t Loss: 101.249062\n",
      "Train Epoch: 20 [30000/60000 (50%)]\t Loss: 101.455908\n",
      "Train Epoch: 20 [40000/60000 (67%)]\t Loss: 99.548828\n",
      "Train Epoch: 20 [50000/60000 (83%)]\t Loss: 105.663887\n",
      "======> Epoch: 20 Average loss: 101.3294\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\t Loss: 100.186357\n",
      "Train Epoch: 21 [10000/60000 (17%)]\t Loss: 98.251553\n",
      "Train Epoch: 21 [20000/60000 (33%)]\t Loss: 102.215059\n",
      "Train Epoch: 21 [30000/60000 (50%)]\t Loss: 98.077773\n",
      "Train Epoch: 21 [40000/60000 (67%)]\t Loss: 106.415996\n",
      "Train Epoch: 21 [50000/60000 (83%)]\t Loss: 99.925410\n",
      "======> Epoch: 21 Average loss: 101.0995\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\t Loss: 98.904434\n",
      "Train Epoch: 22 [10000/60000 (17%)]\t Loss: 98.961670\n",
      "Train Epoch: 22 [20000/60000 (33%)]\t Loss: 100.937734\n",
      "Train Epoch: 22 [30000/60000 (50%)]\t Loss: 107.502500\n",
      "Train Epoch: 22 [40000/60000 (67%)]\t Loss: 102.474941\n",
      "Train Epoch: 22 [50000/60000 (83%)]\t Loss: 102.332588\n",
      "======> Epoch: 22 Average loss: 100.9502\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\t Loss: 105.504297\n",
      "Train Epoch: 23 [10000/60000 (17%)]\t Loss: 97.281953\n",
      "Train Epoch: 23 [20000/60000 (33%)]\t Loss: 101.188809\n",
      "Train Epoch: 23 [30000/60000 (50%)]\t Loss: 101.130313\n",
      "Train Epoch: 23 [40000/60000 (67%)]\t Loss: 105.303535\n",
      "Train Epoch: 23 [50000/60000 (83%)]\t Loss: 104.872637\n",
      "======> Epoch: 23 Average loss: 100.9020\n",
      "\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\t Loss: 99.057295\n",
      "Train Epoch: 24 [10000/60000 (17%)]\t Loss: 99.153730\n",
      "Train Epoch: 24 [20000/60000 (33%)]\t Loss: 100.347051\n",
      "Train Epoch: 24 [30000/60000 (50%)]\t Loss: 98.347842\n",
      "Train Epoch: 24 [40000/60000 (67%)]\t Loss: 100.935352\n",
      "Train Epoch: 24 [50000/60000 (83%)]\t Loss: 99.739590\n",
      "======> Epoch: 24 Average loss: 100.8020\n",
      "\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\t Loss: 97.917979\n",
      "Train Epoch: 25 [10000/60000 (17%)]\t Loss: 101.882207\n",
      "Train Epoch: 25 [20000/60000 (33%)]\t Loss: 100.142871\n",
      "Train Epoch: 25 [30000/60000 (50%)]\t Loss: 98.625273\n",
      "Train Epoch: 25 [40000/60000 (67%)]\t Loss: 105.062324\n",
      "Train Epoch: 25 [50000/60000 (83%)]\t Loss: 100.814854\n",
      "======> Epoch: 25 Average loss: 100.6959\n",
      "\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\t Loss: 101.075664\n",
      "Train Epoch: 26 [10000/60000 (17%)]\t Loss: 104.326572\n",
      "Train Epoch: 26 [20000/60000 (33%)]\t Loss: 102.614717\n",
      "Train Epoch: 26 [30000/60000 (50%)]\t Loss: 101.005322\n",
      "Train Epoch: 26 [40000/60000 (67%)]\t Loss: 99.793867\n",
      "Train Epoch: 26 [50000/60000 (83%)]\t Loss: 103.457920\n",
      "======> Epoch: 26 Average loss: 100.4830\n",
      "\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\t Loss: 98.131406\n",
      "Train Epoch: 27 [10000/60000 (17%)]\t Loss: 104.116260\n",
      "Train Epoch: 27 [20000/60000 (33%)]\t Loss: 96.788105\n",
      "Train Epoch: 27 [30000/60000 (50%)]\t Loss: 100.144395\n",
      "Train Epoch: 27 [40000/60000 (67%)]\t Loss: 100.345352\n",
      "Train Epoch: 27 [50000/60000 (83%)]\t Loss: 101.248457\n",
      "======> Epoch: 27 Average loss: 100.3253\n",
      "\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\t Loss: 98.657754\n",
      "Train Epoch: 28 [10000/60000 (17%)]\t Loss: 101.651611\n",
      "Train Epoch: 28 [20000/60000 (33%)]\t Loss: 101.146260\n",
      "Train Epoch: 28 [30000/60000 (50%)]\t Loss: 101.912637\n",
      "Train Epoch: 28 [40000/60000 (67%)]\t Loss: 100.109639\n",
      "Train Epoch: 28 [50000/60000 (83%)]\t Loss: 101.529873\n",
      "======> Epoch: 28 Average loss: 100.3348\n",
      "\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\t Loss: 99.888320\n",
      "Train Epoch: 29 [10000/60000 (17%)]\t Loss: 99.694746\n",
      "Train Epoch: 29 [20000/60000 (33%)]\t Loss: 98.768906\n",
      "Train Epoch: 29 [30000/60000 (50%)]\t Loss: 91.241885\n",
      "Train Epoch: 29 [40000/60000 (67%)]\t Loss: 95.701074\n",
      "Train Epoch: 29 [50000/60000 (83%)]\t Loss: 96.840283\n",
      "======> Epoch: 29 Average loss: 100.2811\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 코드 13-20 모델 학습\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(0, epochs)):\n",
    "    train(epoch, model, train_loader, optimizer)\n",
    "    test(epoch, model, test_loader)\n",
    "    print(\"\\n\")\n",
    "writer.close()  # ①"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 78976), started 0:00:05 ago. (Use '!kill 78976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-84b7a8162597a047\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-84b7a8162597a047\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 코드 13-21 텐서보드에서 오차 확인\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir scalar --port=6013"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
